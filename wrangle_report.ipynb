{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wrangle_report\n",
    "#### by Yves Deutschmann\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This report will give an brief description of my efforts wrangling the WeRateDogs dataset for the Nanodegree program of Udacity. The report is strucured as follows:\n",
    "\n",
    "1. Gathering the data\n",
    "2. Assessment of the dataset\n",
    "3. Cleaning of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the data  \n",
    "\n",
    "For this project we have 3 sources of data:\n",
    "- the WeRateDogs twitter archive\n",
    "- image predictions for each tweet\n",
    "- retweet count and favorite count for each tweet\n",
    "\n",
    "### WeRateDogs twitter archive:\n",
    "The Twitter archive is provided by Udacity on their website and was downloaded manually. The file was stored as `twitter-archive-enhanced.csv` and processed with the pandas `pd.read_csv()` method that forms the `archive`-DataFrame.  \n",
    "\n",
    "### image predictions:\n",
    "The image predictions file contains a table that provides predictions for what is to see on the image of each tweet in the Twitter archive. These predictions were the results of a neural network. Udacity hosted this file on their servers and we downloaded the file programmatically with the `requests` library. After that, we read in the `image-predictions.tsv` file with the same `read_csv()` method as above. This time we had to specify the separator as `/t` (tab stop separated) which results in the `images`-DataFrame \n",
    "\n",
    "### retweet and favorite count:\n",
    "This information is acquired through the official Twitter API. We set up a Twitter developer account and set up our python `tweepy` library with this authentification information. For every tweet_id in the Twitter archive we requested the original tweet information in `.json`format and wrote each json-package in one line of a new text-file named `tweet_json.txt`. This text-file is processed with the `pd.read_csv()` method forming the `tweets`-DataFrame.\n",
    "\n",
    "### Gathering - Conclusion:\n",
    "We now have three DataFrames that are ready for Assessment in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessment of the dataset\n",
    "\n",
    "Now that we have gathered all of our data we will assess our DataFrames for issues in quality and tidiness. Udacity is supporting us with the following **Key Points**, that we should keep in mind:\n",
    "\n",
    ">- You only want original ratings (no retweets) that have images. Though there are 5000+ tweets in the dataset, not all are dog ratings and some are retweets.\n",
    ">- Assessing and cleaning the entire dataset completely would require a lot of time, and is not necessary to practice and demonstrate your skills in data wrangling. Therefore, the requirements of this project are only to assess and clean at least 8 quality issues and at least 2 tidiness issues in this dataset.\n",
    ">- Cleaning includes merging individual pieces of data according to the rules of tidy data.\n",
    ">- The fact that the rating numerators are greater than the denominators does not need to be cleaned. This unique rating system is a big part of the popularity of WeRateDogs.\n",
    ">- You do not need to gather the tweets beyond August 1st, 2017. You can, but note that you won't be able to gather the image predictions for these tweets since you don't have access to the algorithm used.\n",
    "\n",
    "The following issues were found in our tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`archive`:\n",
    "#### Tidiness:\n",
    "- dog-stage should be a variable (*doggo, floofer, pupper, puppo*)\n",
    "- `nominator`and `denominator`should be one variable\n",
    "- columns for retweets are useless after removing retweeted rows\n",
    "\n",
    "#### Quality: \n",
    "- `twitter_id`s are `int` instead of `string`\n",
    "- Erroneous datatypes for timestamp\n",
    "- Erroneous datatypes for dog-stage\n",
    "- some ratings don't have a related image\n",
    "- data includes rows from retweets\n",
    "- wrong ratings\n",
    "- wrong and missing names\n",
    "- data in `source` is HTML code\n",
    "\n",
    "`images`:\n",
    "#### Tidiness:\n",
    "- table should be part of `archive`\n",
    "\n",
    "#### Quality:\n",
    "- `twitter_id` is `int` instead of `string`\n",
    "\n",
    "`tweets`:\n",
    "#### Tidiness:\n",
    "- should be part of `archive`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cleaning the dataset\n",
    "\n",
    "Before we start to manipulate our data we made a copy of each DataFrame to avoid the loss of data. The next step of the wrangling process is to clean the issues that we found in the assessment. For each issue in or findings list above we will take the following steps to clean the data:\n",
    "- *Define* the steps we will take to tackle the obstacle\n",
    "- transform the described steps  into *Code*\n",
    "- *Test* if the taken actions led to the desired result\n",
    "\n",
    "### Cleaning - Conclusion\n",
    "For this project, we didn't address all issues in quality and tidiness of this dataset as this would consume too much time. Though, we did address all Key Points mentioned above. The result is a single DataFrame that we stored in the file `twitter_archive_master.csv`. With this file, we are ready to analyze the dataset in an efficient and structured way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Over the course of this project, we've seen that data isn't clean at all. If you get clean data, chances are that somebody else already had taken the time to do the job of data wrangling. But the effort is worth it. With our actions, we were able to normalize the rating to a consistent scale that can quickly be processed for aggregation and visualization. Furthermore, data wrangling helps to avoid mistakes that could lead to false assumptions and models.  \n",
    "So, in conclusion, I can say that before you start analyzing, you should always wrangle your data, to not give yourself a hard time dodging issues in your data later off."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
